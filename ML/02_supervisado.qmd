---
title: "Clase 2: Aprendizaje Supervisado"
author: 
- Pamela E. Pairo
- Posgrado Digital Accounting
title-slide-attributes:
    data-background-image: "img/portada.jpg"
format: 
  revealjs:
    theme: [mytheme.scss]
    slide-number: c
    incremental: false
    width: 1600
    height: 1000
    logo: "img/logo.jpg"
    footer: "[Aprendizaje Supervisado- PDA](https://github.com/PamelaPairo/clases_pda)"
    chalkboard:
      theme: whiteboard
      boardmarker-width: 5
      buttons: true
    echo: true
editor: visual
---

## En la clase de hoy...

[**En la primera parte:**]{style="color: #ae260e;"}

. . .

-   츼rboles de decisi칩n
-   Ensambles: Bagging y Boosting
-   Naive Nayes

. . .

[**Recreo**]{style="color: #ae260e;"} `r emo::ji("coffee")`游븰

. . .

[**En la segunda parte:**]{style="color: #ae260e;"}

. . .

-   Support Vector Machine (SVM)
-   Ejercitaci칩n

## {background-color="#C5E1A5"}

<h3 style="color:black;background-color: rgba(255,255,255,0.65);padding:5px;line-height:2em; text-align: center; position: absolute; top: 40%; width: 100%;">

[츼rboles de decisi칩n 游쑇{style="font-size: 100px;"}

</h3>

## 츼rboles de decisi칩n {chalkboard-buttons="true"}

::: columns
::: {.column width="50%"}
![](https://raw.githubusercontent.com/PamelaPairo/maestria_DM/main/aprendizaje_automatico/images/arbol.png){.fragment width="850" fig-align="center"}
:::

::: {.column width="50%"}
::: incremental
-   Aprende a diferenciar los datos en base a [**reglas de decisi칩n**]{style="color: #88188a;"}.

-   Funcionan bien para datos no linealmente separables.

-   Cada nodo interno compara un atributo xi

-   Una rama por cada valor de atributo xi=v

-   Cada hoja asigna una clase y

-   춰Modelo interpretable!
:::
:::
:::

## 쮺on qu칠 atributo comenzar?

![](https://raw.githubusercontent.com/PamelaPairo/maestria_DM/main/aprendizaje_automatico/images/arbol2.png){fig-align="center"}

> Una partici칩n es buena si estamos "m치s seguros" de la clasificaci칩n despu칠s de haberla realizado

## 쯈u칠 atributo es el mejor?

[**Entrop칤a**]{style="color: #88188a;"}

Medida de incertidumbre de una variable aleatoria

$\LARGE H(Y) = -\sum_{i=1}^{k}p(Y=y_i)log_2 p(Y=y_i)$

. . .

::: {style="text-align: center;"}
> **A mayor incertidumbre, mayor entrop칤a**
:::

. . .

[**Entrop칤a Condicional**]{style="color: #88188a;"}

$H(Y|X) = -\sum_{j=1}^{v}p(X=x_j)\sum_{i=1}^{k}p(Y=y_i|X=x_j)log_2 p(Y=y_i| X=x_j)$

## 쯈u칠 atributo es el mejor?

[**Ganancia de Informaci칩n**]{style="color: #88188a;"}

Decrecimiento de entrop칤a (incertidumbre) luego de la partici칩n

::: {style="text-align: center;"}
$\LARGE IG(X)= H(Y)- H(Y|X)$

> [**Elijo el atributo que me de mayor ganancia de informaci칩n**]{style="font-size: 55px;"}
:::

## Aprendizaje en 츼rboles de decisi칩n

::: columns
::: {.column width="50%"}
![](https://raw.githubusercontent.com/PamelaPairo/Taller_IA/main/clases/images/iris.png){fig-align="center"}
:::

::: {.column width="50%"}
![](https://raw.githubusercontent.com/PamelaPairo/Taller_IA/main/clases/images/iris_plot.png){fig-align="center"}
:::
:::

::: incremental
-   Comenzar con un 치rbol vac칤o

-   Generar una partici칩n usando siguiente mejor atributo

-   Usar, por ejemplo, ganancia de informaci칩n y realizar el paso anterior de manera recursiva
:::

## 쮿asta cuando dejo de "partir" un nodo?

> [[**Hiperpar치metro**]{style="color: #88188a;"}: Valores no aprendidos por el algoritmo desde los datos y por ende deben ser seteados antes de entrenar el algoritmo.]{style="font-size: 50px; padding:15px"}

### En 치rboles de decisi칩n:

 `n_min` : n m칤nimo para dividir los nodos

 `tree_depth`: l칤mite a la profundidad del 치rbol

 `cost_complexity`: costo o penalizaci칩n a los errores de 치rboles m치s complejos. Es una forma de poda.

## Sobreajuste

- El error de entrenamiento es siempre cero 

- Poca capacidad de generalizaci칩n.

![](https://raw.githubusercontent.com/PamelaPairo/maestria_DM/main/aprendizaje_automatico/images/overfitting.png){fig-align="center"}

> [El objetivo de un modelo de Machine Learning es que generalice bien frente a nuevos datos o datos no vistos por el modelo (set de testeo)]{style="font-size: 55px; padding:15px"}

::: footer
Imagen extra칤da de [aqu칤](https://bookdown.org/content/2031/arboles-de-decision-parte-i.html)
:::

## Sesgo y varianza

![](https://raw.githubusercontent.com/PamelaPairo/Taller_IA/main/clases/images/sesgo.png){fig-align="center"}

El sesgo (o bias) es la diferencia entre el valor medio predicho por el modelo y el valor medio real.

La varianza se puede relacionar con la complejidad de los modelos. A medida que aumenta la complejidad, aumentan las posibilidades de sobreajuste, es decir, la varianza aumenta.

## 

[**Ventajas `r emo::ji("bulb")`**]{style="color: #88188a; font-size: 55px"}

- F치cil de entender
- Util en exploraci칩n de datos:identificar importancia de variables a partir de cientos de variables.
- El tipo de datos no es una restricci칩n
- Es un m칠todo no param칠trico (i.e., no hay suposici칩n acerca del espacio de distribuci칩n y la estructura del clasificador)

. . .

[**Desventajas `r emo::ji("bulb")`**]{style="color: #88188a; font-size: 55px"}

- Sobreajuste
- P칠rdida de informaci칩n al categorizar variables continuas
- Inestables debido a que peque침as variaciones en el dataset pueden generar modelos muy diferentes.

## {background-color="#C5E1A5"}

<h3 style="color:black;background-color: rgba(255,255,255,0.65);padding:5px;line-height:2em; text-align: center; position: absolute; top: 40%; width: 100%;">

[Ensamble Learning 游쓇릛쓇릛쑇{style="font-size: 100px;"}

</h3>

## Ensamble learning: Motivaci칩n

Se basan en la idea de que el trabajo en conjunto deber칤a dar mejores resultados.

Habitualmente, un modelo "ensemble" es m치s preciso que los modelos que lo constituyen. Intuitivamente, esto se debe a que "dos aprenden mejor que uno".

. . .

> Con los m칠todos de ensemble se pueden combinar m칰ltiples modelos en uno nuevo y as칤 lograr un equilibro entre [**sesgo y varianza**]{style="color: #88188a;"}, y por ende conseguir mejores predicciones que cualquiera de los modelos individuales originales

## Ensemble learning: Bagging

Los [**치rboles de decisi칩n**]{style="color: #88188a;"} son algoritmos inestables debido a que peque침as variaciones en el dataset pueden generar modelos muy diferentes.

. . .

> [**Bagging (Bootstrap Aggregation)**]{style="color: #88188a;"} es un m칠todo para hacer aprendizaje por _ensemble_. Consiste en realizar K subsets del dataset aleatoriamente y con reemplazo, resultando en un ensamble de K modelos. La asignaci칩n de la clase se realiza por mayoria simple en casos de clasificaci칩n.

## Ensemble learning: Bagging

![](https://raw.githubusercontent.com/PamelaPairo/maestria_DM/main/aprendizaje_automatico/images/bagging.png){fig-align="center"}

:::footer
Imagen de Hendrik Blockeel
:::

## Random Forest

- Es una modificaci칩n a Bagging para 츼rboles de Decisi칩n.

- En cada 치rbol se consideran s칩lo M atributos elegidos aleatoriamente.

- El algoritmo es sencillo, f치cil de implementar, f치cil de usar y requiere de poco ajuste de par치metros.

- Es menos interpretable que los 치rboles de decisi칩n.

![](https://raw.githubusercontent.com/PamelaPairo/Taller_IA/main/clases/images/random_forest.png){fig-align="center"}

## Ensamble learning: Boosting

![](https://raw.githubusercontent.com/PamelaPairo/maestria_DM/main/aprendizaje_automatico/images/boosting.png){fig-align="center"}

:::footer

Imagen extra칤da de [aqu칤](https://datascience.eu/machine-learning/gradient-boosting-what-you-need-to-know/)

:::

## Bagging vs Boosting

![](https://raw.githubusercontent.com/PamelaPairo/maestria_DM/main/aprendizaje_automatico/images/bag_vs_boost.jpeg){fig-align="center"}

:::footer

Imagen extra칤da de [aqu칤](https://towardsdatascience.com/ensemble-learning-bagging-boosting-3098079e5422)
:::

## {background-color="#C5E1A5"}

<h3 style="color:black;background-color: rgba(255,255,255,0.65);padding:5px;line-height:2em; text-align: center; position: absolute; top: 40%; width: 100%;">

[Naive Bayes 九]{style="font-size: 100px;"}

</h3>

## Teorema de Bayes {.center-x}

$\LARGE P(A|B)= \frac{P(B|A) * P(A)}{P(B)}$

> El teorema establece que se puede encontrar la probabilidad de **A** (e.g. una clase objetivo) dada la ocurrencia de B (e.g. un conjunto de features). Es decir, B es la evidencia y A es la hip칩tesis.

Dada nuestras variables predictoras, 쯖u치l es la probabilidad de cada clase?

$\ P(Clase|Predictores)= \LARGE \frac{P(Predictores|Clase) * P(Clase)}{P(Predictores)}$

$\LARGE = \frac{Prior * Likelihood}{Evidencia}$

$\ Predictores= \LARGE (x_1, x_2, x_3...x_n)$

## 쯇orqu칠 Naive?

La principal asunci칩n es que [**los atributos son independientes entre s칤.**]{style="color: #88188a;"}

Una segunda asunci칩n, es que [**todos los atributos tienen el mismo efecto en la salida del algoritmo.**]{style="color: #88188a;"}

### Entonces...


$\ P(y|x_1, x_2..x_n)= \LARGE \frac{P(x_1|y) * P(x_n|y)...P(x_1|y)* P(y)}{P(x_1)* P(x_2)...P(x_N)}$


## Tipos de algoritmos

[**Bernoulli Naive Bayes**]{style="color: #88188a;"}: Para casos donde los atributos son variables binarias (e.g. si una palabra ocurre o no en un documento).


[**Multinomial Naive Bayes**]{style="color: #88188a;"}: Para casos donde los atributos representan frecuencias (e.g. la cantidad de veces que una palabra ocurre en un documento).


[**Gaussian Naive Bayes**]{style="color: #88188a;"}: Para casos donde los atributos toman valores continuos, se asume que los valores son muestras de una distribuci칩n gaussiana (esto se usa para calcular las probabilidades condicionales en el algoritmo).

##

```{R}
#| echo: false
library(countdown)
countdown(minutes = 10, 
          right = "20%",
          bottom= "35%",
          padding = "50px",
          margin = "4%",
          font_size = "7em",
          color_border      = "#d33682", #solarized$magenta,
  color_text                = "#d33682", #solarized$magenta,
  color_running_text        = "#073642", #solarized$base02,
  color_finished_background = "#dc322f", #solarized$red,
  color_finished_text       = "#fdf6e3"  #solarized$base3
          )
```

##  {background-image="img/computadora.jpg" background-size="cover"}

<h3 style="color:black;background-color: rgba(255,255,255,0.65);padding:15px;line-height:2em; text-align: center; position: absolute; top: 25%; width: 56%;">

[춰Manos a R!]{style="font-size: 100px;"}

</h3>

::: footer
Foto de <a href="https://unsplash.com/@emilep?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Emile Perron</a> en <a href="https://unsplash.com/es?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Unsplash</a>
:::

## Referencias

-   [Applied Machine Learning](https://rstudio-conf-2020.github.io/applied-ml/Part_6.html#1) dictado en rstudio::conf 2020

-   [Data preprocessing and resampling using tidymodels](https://www.youtube.com/watch?v=s3TkvZM60iU&ab_channel=JuliaSilge), tutorial en Youtube de Julia Silge
