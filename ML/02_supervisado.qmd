---
title: "Clase 2: Aprendizaje Supervisado"
author: 
- Pamela E. Pairo
- Posgrado Digital Accounting
title-slide-attributes:
    data-background-image: "img/portada.jpg"
format: 
  revealjs:
    theme: [mytheme.scss]
    slide-number: c
    incremental: false
    width: 1600
    height: 1000
    logo: "img/logo.jpg"
    footer: "[Aprendizaje Supervisado- PDA](https://github.com/PamelaPairo/clases_pda)"
    chalkboard:
      theme: whiteboard
      boardmarker-width: 5
      buttons: false
    echo: true
editor: visual
---

## En la clase de hoy...

[**En la primera parte:**]{style="color: #ae260e;"}

. . .

-   rboles de decisi贸n
-   Ensambles: Bagging y Boosting
-   SVM

. . .

[**Recreo**]{style="color: #ae260e;"} `r emo::ji("coffee")`

. . .

[**En la segunda parte:**]{style="color: #ae260e;"}

. . .

-   Naive Bayes
-   Ejercitaci贸n

## rboles de decisi贸n {chalkboard-buttons="true"}

::: columns
::: {.column width="50%"}
![](https://raw.githubusercontent.com/PamelaPairo/maestria_DM/main/aprendizaje_automatico/images/arbol.png){.fragment width="850" fig-align="center"}
:::

::: {.column width="50%"}
::: incremental
-   Aprende a diferenciar los datos en base a [**reglas de decisi贸n**]{style="color: #88188a;"}.

-   Funcionan bien para datos no linealmente separables.

-   Cada nodo interno compara un atributo xi

-   Una rama por cada valor de atributo xi=v

-   Cada hoja asigna una clase y

-   隆Modelo interpretable!
:::
:::
:::

## 驴Con qu茅 atributo comenzar?

![](https://raw.githubusercontent.com/PamelaPairo/maestria_DM/main/aprendizaje_automatico/images/arbol2.png){fig-align="center"}

> Una partici贸n es buena si estamos "m谩s seguros" de la clasificaci贸n despu茅s de haberla realizado

## 驴Qu茅 atributo es el mejor?

[**Entrop铆a**]{style="color: #88188a;"}

Medida de incertidumbre de una variable aleatoria

$\LARGE H(Y) = -\sum_{i=1}^{k}p(Y=y_i)log_2 p(Y=y_i)$

. . .

::: {style="text-align: center;"}
> **A mayor incertidumbre, mayor entrop铆a**
:::

. . .

[**Entrop铆a Condicional**]{style="color: #88188a;"}

$H(Y|X) = -\sum_{j=1}^{v}p(X=x_j)\sum_{i=1}^{k}p(Y=y_i|X=x_j)log_2 p(Y=y_i| X=x_j)$

## 驴Qu茅 atributo es el mejor?

[**Ganancia de Informaci贸n**]{style="color: #88188a;"}

Decrecimiento de entrop铆a (incertidumbre) luego de la partici贸n

::: {style="text-align: center;"}
$\LARGE IG(X)= H(Y)- H(Y|X)$

> [**Elijo el atributo que me de mayor ganancia de informaci贸n**]{style="font-size: 55px;"}
:::

## Aprendizaje en rboles de decisi贸n {chalkboard-buttons="true"}

::: columns
::: {.column width="50%"}
![](https://raw.githubusercontent.com/PamelaPairo/Taller_IA/main/clases/images/iris.png){fig-align="center"}
:::

::: {.column width="50%"}
![](https://raw.githubusercontent.com/PamelaPairo/Taller_IA/main/clases/images/iris_plot.png){fig-align="center"}
:::
:::

::: incremental
-   Comenzar con un 谩rbol vac铆o

-   Generar una partici贸n usando siguiente mejor atributo

-   Usar, por ejemplo, ganancia de informaci贸n y realizar el paso anterior de manera recursiva
:::

## 驴Hasta cuando dejo de "partir" un nodo?

> [[**Hiperpar谩metro**]{style="color: #88188a;"}: Valores no aprendidos por el algoritmo desde los datos y por ende deben ser seteados antes de entrenar el algoritmo.]{style="font-size: 50px; padding:15px"}

## Sobreajuste

- El error de entrenamiento es siempre cero 

- Poca capacidad de generalizaci贸n.

![](https://raw.githubusercontent.com/PamelaPairo/maestria_DM/main/aprendizaje_automatico/images/overfitting.png){fig-align="center"}

> [El objetivo de un modelo de Machine Learning es que generalice bien frente a nuevos datos o datos no vistos por el modelo (set de testeo)]{style="font-size: 55px; padding:15px"}

::: footer
Imagen extra铆da de [aqu铆](https://bookdown.org/content/2031/arboles-de-decision-parte-i.html)
:::

## 

[**Ventajas `r emo::ji("bulb")`**]{style="color: #88188a; font-size: 55px"}

- F谩cil de entender
- Util en exploraci贸n de datos:identificar importancia de variables a partir de cientos de variables.
- El tipo de datos no es una restricci贸n
- Es un m茅todo no param茅trico (i.e., no hay suposici贸n acerca del espacio de distribuci贸n y la estructura del clasificador)

. . .

[**Desventajas `r emo::ji("bulb")`**]{style="color: #88188a; font-size: 55px"}

- Sobreajuste
- P茅rdida de informaci贸n al categorizar variables continuas
- Inestables debido a que peque帽as variaciones en el dataset pueden generar modelos muy diferentes.

## Ensamble learning: Motivaci贸n

Se basan en la idea de que el trabajo en conjunto deber铆a dar mejores resultados.

Habitualmente, un modelo "ensemble" es m谩s preciso que los modelos que lo constituyen. Intuitivamente, esto se debe a que "dos aprenden mejor que uno".

. . .

> Con los m茅todos de ensemble se pueden combinar m煤ltiples modelos en uno nuevo y as铆 lograr un equilibro entre [**sesgo y varianza**]{style="color: #88188a;"}, y por ende conseguir mejores predicciones que cualquiera de los modelos individuales originales

## 

```{R}
#| echo: false
library(countdown)
countdown(minutes = 10, 
          right = "20%",
          bottom= "35%",
          padding = "50px",
          margin = "4%",
          font_size = "7em",
          color_border      = "#d33682", #solarized$magenta,
  color_text                = "#d33682", #solarized$magenta,
  color_running_text        = "#073642", #solarized$base02,
  color_finished_background = "#dc322f", #solarized$red,
  color_finished_text       = "#fdf6e3"  #solarized$base3
          )
```

##  {background-image="img/computadora.jpg" background-size="cover"}

<h3 style="color:black;background-color: rgba(255,255,255,0.65);padding:15px;line-height:2em; text-align: center; position: absolute; top: 25%; width: 56%;">

[隆Manos a R!]{style="font-size: 100px;"}

</h3>

::: footer
Foto de <a href="https://unsplash.com/@emilep?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Emile Perron</a> en <a href="https://unsplash.com/es?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Unsplash</a>
:::

## Referencias

-   [Applied Machine Learning](https://rstudio-conf-2020.github.io/applied-ml/Part_6.html#1) dictado en rstudio::conf 2020

-   [Data preprocessing and resampling using tidymodels](https://www.youtube.com/watch?v=s3TkvZM60iU&ab_channel=JuliaSilge), tutorial en Youtube de Julia Silge
