---
title: "Clase 2: Aprendizaje Supervisado"
author: 
- Pamela E. Pairo
- Posgrado Digital Accounting
title-slide-attributes:
    data-background-image: "img/portada.jpg"
format: 
  revealjs:
    theme: [mytheme.scss]
    slide-number: c
    incremental: false
    width: 1600
    height: 1000
    logo: "img/logo.jpg"
    footer: "[Aprendizaje Supervisado- PDA](https://github.com/PamelaPairo/clases_pda)"
    chalkboard:
      theme: whiteboard
      boardmarker-width: 5
      buttons: false
    echo: true
editor: visual
---

## En la clase de hoy...

[**En la primera parte:**]{style="color: #ae260e;"}

. . .

-   Árboles de decisión
-   Ensambles: Bagging y Boosting
-   SVM

. . .

[**Recreo**]{style="color: #ae260e;"} `r emo::ji("coffee")`🧉

. . .

[**En la segunda parte:**]{style="color: #ae260e;"}

. . .

-   Naive Bayes
-   Ejercitación

## Árboles de decisión {chalkboard-buttons="true"}

::: columns
::: {.column width="50%"}
![](https://raw.githubusercontent.com/PamelaPairo/maestria_DM/main/aprendizaje_automatico/images/arbol.png){.fragment width="850" fig-align="center"}
:::

::: {.column width="50%"}
::: incremental
-   Aprende a diferenciar los datos en base a [**reglas de decisión**]{style="color: #88188a;"}.

-   Funcionan bien para datos no linealmente separables.

-   Cada nodo interno compara un atributo xi

-   Una rama por cada valor de atributo xi=v

-   Cada hoja asigna una clase y

-   ¡Modelo interpretable!
:::
:::
:::

## ¿Con qué atributo comenzar?

![](https://raw.githubusercontent.com/PamelaPairo/maestria_DM/main/aprendizaje_automatico/images/arbol2.png){fig-align="center"}

> Una partición es buena si estamos "más seguros" de la clasificación después de haberla realizado

## ¿Qué atributo es el mejor?

[**Entropía**]{style="color: #88188a;"}

Medida de incertidumbre de una variable aleatoria

$\LARGE H(Y) = -\sum_{i=1}^{k}p(Y=y_i)log_2 p(Y=y_i)$

. . .

::: {style="text-align: center;"}
> **A mayor incertidumbre, mayor entropía**
:::

. . .

[**Entropía Condicional**]{style="color: #88188a;"}

$H(Y|X) = -\sum_{j=1}^{v}p(X=x_j)\sum_{i=1}^{k}p(Y=y_i|X=x_j)log_2 p(Y=y_i| X=x_j)$

## ¿Qué atributo es el mejor?

[**Ganancia de Información**]{style="color: #88188a;"}

Decrecimiento de entropía (incertidumbre) luego de la partición

::: {style="text-align: center;"}
$\LARGE IG(X)= H(Y)- H(Y|X)$

> [**Elijo el atributo que me de mayor ganancia de información**]{style="font-size: 55px;"}
:::

## Aprendizaje en Árboles de decisión {chalkboard-buttons="true"}

::: columns
::: {.column width="50%"}
![](https://raw.githubusercontent.com/PamelaPairo/Taller_IA/main/clases/images/iris.png){fig-align="center"}
:::

::: {.column width="50%"}
![](https://raw.githubusercontent.com/PamelaPairo/Taller_IA/main/clases/images/iris_plot.png){fig-align="center"}
:::
:::

::: incremental
-   Comenzar con un árbol vacío

-   Generar una partición usando siguiente mejor atributo

-   Usar, por ejemplo, ganancia de información y realizar el paso anterior de manera recursiva
:::

## ¿Hasta cuando dejo de "partir" un nodo?

> [[**Hiperparámetro**]{style="color: #88188a;"}: Valores no aprendidos por el algoritmo desde los datos y por ende deben ser seteados antes de entrenar el algoritmo.]{style="font-size: 50px; padding:15px"}

## Sobreajuste

- El error de entrenamiento es siempre cero 

- Poca capacidad de generalización.

![](https://raw.githubusercontent.com/PamelaPairo/maestria_DM/main/aprendizaje_automatico/images/overfitting.png){fig-align="center"}

> [El objetivo de un modelo de Machine Learning es que generalice bien frente a nuevos datos o datos no vistos por el modelo (set de testeo)]{style="font-size: 55px; padding:15px"}

::: footer
Imagen extraída de [aquí](https://bookdown.org/content/2031/arboles-de-decision-parte-i.html)
:::

## 

[**Ventajas `r emo::ji("bulb")`**]{style="color: #88188a; font-size: 55px"}

- Fácil de entender
- Util en exploración de datos:identificar importancia de variables a partir de cientos de variables.
- El tipo de datos no es una restricción
- Es un método no paramétrico (i.e., no hay suposición acerca del espacio de distribución y la estructura del clasificador)

. . .

[**Desventajas `r emo::ji("bulb")`**]{style="color: #88188a; font-size: 55px"}

- Sobreajuste
- Pérdida de información al categorizar variables continuas
- Inestables debido a que pequeñas variaciones en el dataset pueden generar modelos muy diferentes.

## Ensamble learning: Motivación

Se basan en la idea de que el trabajo en conjunto debería dar mejores resultados.

Habitualmente, un modelo "ensemble" es más preciso que los modelos que lo constituyen. Intuitivamente, esto se debe a que "dos aprenden mejor que uno".

. . .

> Con los métodos de ensemble se pueden combinar múltiples modelos en uno nuevo y así lograr un equilibro entre [**sesgo y varianza**]{style="color: #88188a;"}, y por ende conseguir mejores predicciones que cualquiera de los modelos individuales originales

## 

```{R}
#| echo: false
library(countdown)
countdown(minutes = 10, 
          right = "20%",
          bottom= "35%",
          padding = "50px",
          margin = "4%",
          font_size = "7em",
          color_border      = "#d33682", #solarized$magenta,
  color_text                = "#d33682", #solarized$magenta,
  color_running_text        = "#073642", #solarized$base02,
  color_finished_background = "#dc322f", #solarized$red,
  color_finished_text       = "#fdf6e3"  #solarized$base3
          )
```

##  {background-image="img/computadora.jpg" background-size="cover"}

<h3 style="color:black;background-color: rgba(255,255,255,0.65);padding:15px;line-height:2em; text-align: center; position: absolute; top: 25%; width: 56%;">

[¡Manos a R!]{style="font-size: 100px;"}

</h3>

::: footer
Foto de <a href="https://unsplash.com/@emilep?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Emile Perron</a> en <a href="https://unsplash.com/es?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Unsplash</a>
:::

## Referencias

-   [Applied Machine Learning](https://rstudio-conf-2020.github.io/applied-ml/Part_6.html#1) dictado en rstudio::conf 2020

-   [Data preprocessing and resampling using tidymodels](https://www.youtube.com/watch?v=s3TkvZM60iU&ab_channel=JuliaSilge), tutorial en Youtube de Julia Silge
